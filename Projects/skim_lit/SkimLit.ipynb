{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "OS746W3stZSi",
        "-3GKlSBRNldT",
        "Y9iOJJwwNfKG",
        "FhjTnJX4FJy0",
        "EZzeUzIjJsZ6",
        "UJl3HnaeSkE9",
        "qPW5hA5xLCJC",
        "t2-NsuOCLKRe",
        "S-lDr8ERRjtO",
        "2b7I-4Z-SBDV",
        "xBX-_jE84OIb",
        "hPN2cQjsGdth",
        "qIsTKdHCuWpC",
        "z5f3_aAE-oH4",
        "rgX9JK7BQB9J"
      ],
      "authorship_tag": "ABX9TyNZyoY/IJpJ2yD0TptDftNB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Srihari293/AI_ML/blob/main/SkimLit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SkimLit Project \n",
        "Many to one sequence problem"
      ],
      "metadata": {
        "id": "faMf4G14r4KH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Dependencies"
      ],
      "metadata": {
        "id": "BThfQq6Zr78k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bloe-8nNqzw_",
        "outputId": "ebef8f38-ef40-4005-e91a-a5c97e4ffd5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: Tesla T4 (UUID: GPU-3b333689-8b63-da3c-8b26-80fb110150b8)\n"
          ]
        }
      ],
      "source": [
        "# Check for GPU\n",
        "!nvidia-smi -L"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Libraries\n",
        "import string\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from spacy.lang.en import English\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder"
      ],
      "metadata": {
        "id": "CK-j5cbKyaAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Downloading dataset"
      ],
      "metadata": {
        "id": "nlZQNrSxtUHh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Franck-Dernoncourt/pubmed-rct.git\n",
        "!ls pubmed-rct"
      ],
      "metadata": {
        "id": "Ejh0QGbmtXI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check what files are in the PubMed_20K dataset \n",
        "!ls pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign"
      ],
      "metadata": {
        "id": "8gvoBWfyxj1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start by using the 20k dataset\n",
        "data_dir = \"pubmed-rct/PubMed_200k_RCT_numbers_replaced_with_at_sign/\""
      ],
      "metadata": {
        "id": "PJkEQqYWyM-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check all of the filenames in the target directory\n",
        "import os\n",
        "filenames = [data_dir + filename for filename in os.listdir(data_dir)]\n",
        "filenames"
      ],
      "metadata": {
        "id": "pbeGrFQ_yPNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Preprocessing data"
      ],
      "metadata": {
        "id": "OS746W3stZSi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Data extraction"
      ],
      "metadata": {
        "id": "-3GKlSBRNldT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function\n",
        "\n",
        "# Create function to read the lines of a document\n",
        "def get_lines(filename):\n",
        "  \"\"\"\n",
        "  Reads filename (a text file) and returns the lines of text as a list.\n",
        "  \n",
        "  Args:\n",
        "      filename: a string containing the target filepath to read.\n",
        "  \n",
        "  Returns:\n",
        "      A list of strings with one string per line from the target filename.\n",
        "      For example:\n",
        "      [\"this is the first line of filename\",\n",
        "       \"this is the second line of filename\",\n",
        "       \"...\"]\n",
        "  \"\"\"\n",
        "  with open(filename, \"r\") as f:\n",
        "    return f.readlines()"
      ],
      "metadata": {
        "id": "QbzfDFA0tcbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_lines = get_lines(data_dir+\"train.txt\")\n",
        "train_lines[:20] # the whole first example of an abstract + a little more of the next one"
      ],
      "metadata": {
        "id": "ZE9zresGywMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text_with_line_numbers(filename):\n",
        "  \"\"\"Returns a list of dictionaries of abstract line data.\n",
        "\n",
        "  Takes in filename, reads its contents and sorts through each line,\n",
        "  extracting things like the target label, the text of the sentence,\n",
        "  how many sentences are in the current abstract and what sentence number\n",
        "  the target line is.\n",
        "\n",
        "  Args:\n",
        "      filename: a string of the target text file to read and extract line data\n",
        "      from.\n",
        "\n",
        "  Returns:\n",
        "      A list of dictionaries each containing a line from an abstract,\n",
        "      the lines label, the lines position in the abstract and the total number\n",
        "      of lines in the abstract where the line is from. For example:\n",
        "\n",
        "      [{\"target\": 'CONCLUSION',\n",
        "        \"text\": The study couldn't have gone better, turns out people are kinder than you think\",\n",
        "        \"line_number\": 8,\n",
        "        \"total_lines\": 8}]\n",
        "  \"\"\"\n",
        "  # get all lines from filename\n",
        "  input_lines = get_lines(filename) \n",
        "  # create an empty abstract\n",
        "  abstract_lines = \"\"\n",
        "  # create an empty list of abstracts\n",
        "  abstract_samples = [] \n",
        "  \n",
        "  # Loop through each line in target file\n",
        "  for line in input_lines:\n",
        "    # check to see if line is an ID line\n",
        "    if line.startswith(\"###\"):\n",
        "      abstract_id = line\n",
        "      # reset abstract string\n",
        "      abstract_lines = \"\" \n",
        "    \n",
        "    # check to see if line is a new line\n",
        "    elif line.isspace():  \n",
        "      # split abstract into separate lines\n",
        "      abstract_line_split = abstract_lines.splitlines() \n",
        "\n",
        "      # Iterate through each line in abstract and count them at the same time\n",
        "      for abstract_line_number, abstract_line in enumerate(abstract_line_split):\n",
        "        line_data = {}                                   # create empty dict to store data from line\n",
        "        target_text_split = abstract_line.split(\"\\t\")    # split target label from text\n",
        "        line_data[\"target\"] = target_text_split[0]       # get target label\n",
        "        line_data[\"text\"] = target_text_split[1].lower() # get target text and lower it\n",
        "        line_data[\"line_number\"] = abstract_line_number  # what number line does the line appear in the abstract?\n",
        "        line_data[\"total_lines\"] = len(abstract_line_split) - 1 # how many total lines are in the abstract? (start from 0)\n",
        "        abstract_samples.append(line_data) # add line data to abstract samples list\n",
        "    \n",
        "    else: # if the above conditions aren't fulfilled, the line contains a labelled sentence\n",
        "      abstract_lines += line\n",
        "  \n",
        "  return abstract_samples"
      ],
      "metadata": {
        "id": "jOjtTcnwzL6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get data from file and preprocess it\n",
        "%%time\n",
        "train_samples = preprocess_text_with_line_numbers(data_dir + \"train.txt\")\n",
        "val_samples = preprocess_text_with_line_numbers(data_dir + \"dev.txt\") # dev is another name for validation set\n",
        "test_samples = preprocess_text_with_line_numbers(data_dir + \"test.txt\")\n",
        "len(train_samples), len(val_samples), len(test_samples)"
      ],
      "metadata": {
        "id": "jq8LR7aaJgPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the first abstract of our training data\n",
        "train_samples[:14]"
      ],
      "metadata": {
        "id": "h4RW09j7KLH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.DataFrame(train_samples)\n",
        "val_df = pd.DataFrame(val_samples)\n",
        "test_df = pd.DataFrame(test_samples)\n",
        "train_df.head(11)"
      ],
      "metadata": {
        "id": "ZA06gbzHLw8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.target.value_counts()"
      ],
      "metadata": {
        "id": "cnWd9do8L_C2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.total_lines.plot.hist();"
      ],
      "metadata": {
        "id": "_cyWvA7mMGnb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Get list of sentences\n",
        "# Convert abstract text lines into lists \n",
        "train_sentences = train_df[\"text\"].tolist()\n",
        "val_sentences = val_df[\"text\"].tolist()\n",
        "test_sentences = test_df[\"text\"].tolist()\n",
        "len(train_sentences), len(val_sentences), len(test_sentences)"
      ],
      "metadata": {
        "id": "pVALPJCJMtTo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View first 10 lines of training sentences\n",
        "train_sentences[:10]"
      ],
      "metadata": {
        "id": "ImJo5AfwMyym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Numerical labelling"
      ],
      "metadata": {
        "id": "Y9iOJJwwNfKG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# One hot encode labels\n",
        "one_hot_encoder = OneHotEncoder(sparse=False)\n",
        "train_labels_one_hot = one_hot_encoder.fit_transform(train_df[\"target\"].to_numpy().reshape(-1, 1))\n",
        "val_labels_one_hot = one_hot_encoder.transform(val_df[\"target\"].to_numpy().reshape(-1, 1))\n",
        "test_labels_one_hot = one_hot_encoder.transform(test_df[\"target\"].to_numpy().reshape(-1, 1))\n",
        "\n",
        "# Check what training labels look like\n",
        "train_labels_one_hot"
      ],
      "metadata": {
        "id": "BKx7UpuYNpbb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Label encoding the labels\n",
        "label_encoder = LabelEncoder()\n",
        "train_labels_encoded = label_encoder.fit_transform(train_df[\"target\"].to_numpy())\n",
        "val_labels_encoded = label_encoder.transform(val_df[\"target\"].to_numpy())\n",
        "test_labels_encoded = label_encoder.transform(test_df[\"target\"].to_numpy())\n",
        "\n",
        "# Check what training labels look like\n",
        "train_labels_encoded"
      ],
      "metadata": {
        "id": "aomZORaCNvX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get class names and number of classes from LabelEncoder instance \n",
        "num_classes = len(label_encoder.classes_)\n",
        "class_names = label_encoder.classes_\n",
        "num_classes, class_names"
      ],
      "metadata": {
        "id": "7RpJmPa_NvP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Modelling experiments"
      ],
      "metadata": {
        "id": "_ZeCiS4Ntc_t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 Model 0 - Baseline"
      ],
      "metadata": {
        "id": "FhjTnJX4FJy0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a pipeline\n",
        "model_0 = Pipeline([\n",
        "  (\"tf-idf\", TfidfVectorizer()),\n",
        "  (\"clf\", MultinomialNB())\n",
        "])\n",
        "\n",
        "# Fit the pipeline to the training data\n",
        "model_0.fit(X=train_sentences, \n",
        "            y=train_labels_encoded);"
      ],
      "metadata": {
        "id": "c9JSQJpjtf2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate baseline on validation dataset\n",
        "model_0.score(X=val_sentences,\n",
        "              y=val_labels_encoded)"
      ],
      "metadata": {
        "id": "DIpxVSO2IqGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions\n",
        "baseline_preds = model_0.predict(val_sentences)\n",
        "baseline_preds"
      ],
      "metadata": {
        "id": "GmMlJmZNIvJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download helper functions script\n",
        "!wget https://raw.githubusercontent.com/Srihari293/AI_ML/main/Courses/NLP/helper_functions.py"
      ],
      "metadata": {
        "id": "PQLelcPpIyVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import calculate_results helper function\n",
        "from helper_functions import calculate_results"
      ],
      "metadata": {
        "id": "789BsBpSJYKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate baseline results\n",
        "baseline_results = calculate_results(y_true=val_labels_encoded,\n",
        "                                     y_pred=baseline_preds)\n",
        "baseline_results"
      ],
      "metadata": {
        "id": "gdUKxca3JYH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 Preparing text for our deep sequence models"
      ],
      "metadata": {
        "id": "EZzeUzIjJsZ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.2.0 Understanding the data"
      ],
      "metadata": {
        "id": "UJl3HnaeSkE9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# How long is each sentence on average?\n",
        "sent_lens = [len(sentence.split()) for sentence in train_sentences]\n",
        "avg_sent_len = np.mean(sent_lens)\n",
        "avg_sent_len # return average sentence length (in tokens)"
      ],
      "metadata": {
        "id": "DVdDYQ6jJYFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# What's the distribution look like?\n",
        "import matplotlib.pyplot as plt\n",
        "plt.hist(sent_lens, bins=7);"
      ],
      "metadata": {
        "id": "aJUMKIIbJYDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How long of a sentence covers 95% of the lengths?\n",
        "output_seq_len = int(np.percentile(sent_lens, 95))\n",
        "output_seq_len"
      ],
      "metadata": {
        "id": "zT8lxk_uJX_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Maximum sentence length in the training set\n",
        "max(sent_lens)"
      ],
      "metadata": {
        "id": "MY5-XO0mK2gg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.2.1 Creating a text vectorizer"
      ],
      "metadata": {
        "id": "qPW5hA5xLCJC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# How many words are in our vocabulary? (taken from 3.2 in https://arxiv.org/pdf/1710.06071.pdf)\n",
        "max_tokens = 68000"
      ],
      "metadata": {
        "id": "GTa9o4w3MFgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorizer = tf.keras.layers.TextVectorization(max_tokens=max_tokens, # number of words in vocabulary\n",
        "                                    output_sequence_length=55) # desired output length of vectorized sequences"
      ],
      "metadata": {
        "id": "LKzDKjJmMHIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adapt text vectorizer to training sentences\n",
        "text_vectorizer.adapt(train_sentences)"
      ],
      "metadata": {
        "id": "B3BCytE-MHGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_sentence = random.choice(train_sentences)\n",
        "print(f\"Text:\\n{target_sentence}\")\n",
        "print(f\"\\nLength of text: {len(target_sentence.split())}\")\n",
        "print(f\"\\nVectorized text:\\n{text_vectorizer([target_sentence])}\")"
      ],
      "metadata": {
        "id": "dsXoPywdMHEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How many words in our training vocabulary?\n",
        "rct_20k_text_vocab = text_vectorizer.get_vocabulary()\n",
        "print(f\"Number of words in vocabulary: {len(rct_20k_text_vocab)}\"), \n",
        "print(f\"Most common words in the vocabulary: {rct_20k_text_vocab[:5]}\")\n",
        "print(f\"Least common words in the vocabulary: {rct_20k_text_vocab[-5:]}\")"
      ],
      "metadata": {
        "id": "cVNh0f5-MHB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the config of our text vectorizer\n",
        "text_vectorizer.get_config()"
      ],
      "metadata": {
        "id": "U9ekMlE5MiPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.2.2 Create custom text embedding"
      ],
      "metadata": {
        "id": "t2-NsuOCLKRe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create token embedding layer\n",
        "token_embed = layers.Embedding(input_dim=len(rct_20k_text_vocab), # length of vocabulary\n",
        "                               output_dim=128, # Note: different embedding sizes result in drastically different numbers of parameters to train\n",
        "                               # Use masking to handle variable sequence lengths (save space)\n",
        "                               mask_zero=True,\n",
        "                               name=\"token_embedding\") \n",
        "\n",
        "# Show example embedding\n",
        "print(f\"Sentence before vectorization:\\n{target_sentence}\\n\")\n",
        "vectorized_sentence = text_vectorizer([target_sentence])\n",
        "print(f\"Sentence after vectorization (before embedding):\\n{vectorized_sentence}\\n\")\n",
        "embedded_sentence = token_embed(vectorized_sentence)\n",
        "print(f\"Sentence after embedding:\\n{embedded_sentence}\\n\")\n",
        "print(f\"Embedded sentence shape: {embedded_sentence.shape}\")"
      ],
      "metadata": {
        "id": "RU82eHkiMfPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.2.3 Create datasets in TF"
      ],
      "metadata": {
        "id": "S-lDr8ERRjtO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn our data into TensorFlow Datasets\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_sentences, train_labels_one_hot))\n",
        "valid_dataset = tf.data.Dataset.from_tensor_slices((val_sentences, val_labels_one_hot))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_sentences, test_labels_one_hot))\n",
        "\n",
        "train_dataset"
      ],
      "metadata": {
        "id": "GVXC-hJjMfNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the TensorSliceDataset's after prefetching them as batches\n",
        "train_dataset = train_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "valid_dataset = valid_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "test_dataset = test_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "train_dataset "
      ],
      "metadata": {
        "id": "Ljcy6jV6MfLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3 Model 1: CNN with token embeddings (Conv1d)\n",
        "Input (text) -> Tokenize -> Embedding -> Layers -> Output (label probability)"
      ],
      "metadata": {
        "id": "2b7I-4Z-SBDV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create 1D convolutional model to process sequences\n",
        "inputs = layers.Input(shape=(1,), dtype=tf.string)\n",
        "text_vectors = text_vectorizer(inputs)       # vectorize text inputs\n",
        "token_embeddings = token_embed(text_vectors) # create embedding\n",
        "x = layers.Conv1D(64, kernel_size=5, padding=\"same\", activation=\"relu\")(token_embeddings)\n",
        "x = layers.GlobalAveragePooling1D()(x)       # condense the output of our feature vector\n",
        "outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
        "model_1 = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "# Compile\n",
        "model_1.compile(loss=\"categorical_crossentropy\", # if your labels are integer form (not one hot) use sparse_categorical_crossentropy\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "0SmtMeYXSJw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get summary of Conv1D model\n",
        "model_1.summary()"
      ],
      "metadata": {
        "id": "efUoahIXSJuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model\n",
        "model_1_history = model_1.fit(train_dataset,\n",
        "                              steps_per_epoch=int(0.1 * len(train_dataset)), # only fit on 10% of batches for faster training time\n",
        "                              epochs=3,\n",
        "                              validation_data=valid_dataset,\n",
        "                              validation_steps=int(0.1 * len(valid_dataset))) # only validate on 10% of batches"
      ],
      "metadata": {
        "id": "LXdV5DNmSJr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on whole validation dataset (we only validated on 10% of batches during training)\n",
        "model_1.evaluate(valid_dataset)"
      ],
      "metadata": {
        "id": "p9bRoU3N3zv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions (our model outputs prediction probabilities for each class)\n",
        "model_1_pred_probs = model_1.predict(valid_dataset)\n",
        "model_1_pred_probs"
      ],
      "metadata": {
        "id": "-MgR48T53zoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert pred probs to classes\n",
        "model_1_preds = tf.argmax(model_1_pred_probs, axis=1)\n",
        "model_1_preds"
      ],
      "metadata": {
        "id": "8kI5q2ZN3zeu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate model_1 results\n",
        "model_1_results = calculate_results(y_true=val_labels_encoded,\n",
        "                                    y_pred=model_1_preds)\n",
        "model_1_results"
      ],
      "metadata": {
        "id": "ad-k5hka3zZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.4 Model 2: Feature extraction transfer learning model (pre-trained token embeddings):\n",
        "Inputs (string) -> Pretrained embeddings from TensorFlow Hub (Universal Sentence Encoder) -> Layers -> Output (prediction probabilities)"
      ],
      "metadata": {
        "id": "xBX-_jE84OIb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download pretrained TensorFlow Hub USE\n",
        "import tensorflow_hub as hub\n",
        "tf_hub_embedding_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\", trainable=False, name=\"universal_sentence_encoder\")"
      ],
      "metadata": {
        "id": "39IIBB0i4mAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test out the embedding on a random sentence\n",
        "random_training_sentence = random.choice(train_sentences)\n",
        "print(f\"Random training sentence:\\n{random_training_sentence}\\n\")\n",
        "use_embedded_sentence = tf_hub_embedding_layer([random_training_sentence])\n",
        "print(f\"Sentence after embedding:\\n{use_embedded_sentence[0][:30]} (truncated output)...\\n\")\n",
        "print(f\"Length of sentence embedding:\\n{len(use_embedded_sentence[0])}\")"
      ],
      "metadata": {
        "id": "6-eBnWdQ7pNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define feature extractor model using TF Hub layer\n",
        "inputs = layers.Input(shape=[], dtype=tf.string)\n",
        "pretrained_embedding = tf_hub_embedding_layer(inputs)          # tokenize text and create embedding\n",
        "x = layers.Dense(128, activation=\"relu\")(pretrained_embedding) # add a fully connected layer on top of the embedding\n",
        "outputs = layers.Dense(5, activation=\"softmax\")(x)             # create the output layer\n",
        "model_2 = tf.keras.Model(inputs=inputs,\n",
        "                        outputs=outputs)\n",
        "\n",
        "# Compile the model\n",
        "model_2.compile(loss=\"categorical_crossentropy\",\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "6mh0ObiH7pF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a summary of the model\n",
        "model_2.summary()"
      ],
      "metadata": {
        "id": "ooI-U4Im7pDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit feature extractor model for 3 epochs\n",
        "model_2.fit(train_dataset,\n",
        "            steps_per_epoch=int(0.1 * len(train_dataset)),\n",
        "            epochs=3,\n",
        "            validation_data=valid_dataset,\n",
        "            validation_steps=int(0.1 * len(valid_dataset)))"
      ],
      "metadata": {
        "id": "w6nYm6NC7o-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on whole validation dataset\n",
        "model_2.evaluate(valid_dataset)"
      ],
      "metadata": {
        "id": "aY-ROzpm72Bo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions with feature extraction model\n",
        "model_2_pred_probs = model_2.predict(valid_dataset)\n",
        "model_2_pred_probs"
      ],
      "metadata": {
        "id": "uM2hv1UB71-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the predictions with feature extraction model to classes\n",
        "model_2_preds = tf.argmax(model_2_pred_probs, axis=1)\n",
        "model_2_preds"
      ],
      "metadata": {
        "id": "oaYY1G7E716d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate results from TF Hub pretrained embeddings results on validation set\n",
        "model_2_results = calculate_results(y_true=val_labels_encoded,\n",
        "                                    y_pred=model_2_preds)\n",
        "model_2_results"
      ],
      "metadata": {
        "id": "aRe-nEMn7-Di"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.5 Model 3:  CNN with character embeddings (Conv1d)\n",
        "Token level embeddings split sequences into tokens (words) and embeddings each of them, character embeddings split sequences into characters and creates a feature vector for each."
      ],
      "metadata": {
        "id": "hPN2cQjsGdth"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make function to split sentences into characters\n",
        "def split_chars(text):\n",
        "  return \" \".join(list(text))\n",
        "\n",
        "# Test splitting non-character-level sequence into characters\n",
        "split_chars(random_training_sentence)"
      ],
      "metadata": {
        "id": "WY9klPSxGvLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split sequence-level data splits into character-level data splits\n",
        "train_chars = [split_chars(sentence) for sentence in train_sentences]\n",
        "val_chars = [split_chars(sentence) for sentence in val_sentences]\n",
        "test_chars = [split_chars(sentence) for sentence in test_sentences]\n",
        "print(train_chars[0])"
      ],
      "metadata": {
        "id": "fM_awMy9H3LP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# What's the average character length?\n",
        "char_lens = [len(sentence) for sentence in train_sentences]\n",
        "mean_char_len = np.mean(char_lens)\n",
        "mean_char_len"
      ],
      "metadata": {
        "id": "NAwbWqY_Hbhr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the distribution of our sequences at character-level\n",
        "plt.hist(char_lens, bins=7);"
      ],
      "metadata": {
        "id": "AVLf9vbMHeDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find what character length covers 95% of sequences\n",
        "output_seq_char_len = int(np.percentile(char_lens, 95))\n",
        "output_seq_char_len"
      ],
      "metadata": {
        "id": "aB4z1mOmHiSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get all keyboard characters for char-level embedding\n",
        "alphabet = string.ascii_lowercase + string.digits + string.punctuation\n",
        "alphabet"
      ],
      "metadata": {
        "id": "5g6TY_4rHjv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create char-level token vectorizer instance\n",
        "NUM_CHAR_TOKENS = len(alphabet) + 2 # num characters in alphabet + space + OOV token\n",
        "\n",
        "char_vectorizer = tf.keras.layers.TextVectorization(max_tokens=NUM_CHAR_TOKENS,  \n",
        "                                    output_sequence_length=output_seq_char_len,\n",
        "                                    standardize=\"lower_and_strip_punctuation\",\n",
        "                                    name=\"char_vectorizer\")\n",
        "\n",
        "# Adapt character vectorizer to training characters\n",
        "char_vectorizer.adapt(train_chars)"
      ],
      "metadata": {
        "id": "-p_PsEmPHlYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check character vocabulary characteristics\n",
        "char_vocab = char_vectorizer.get_vocabulary()\n",
        "print(f\"Number of different characters in character vocab: {len(char_vocab)}\")\n",
        "print(f\"5 most common characters: {char_vocab[:5]}\")\n",
        "print(f\"5 least common characters: {char_vocab[-5:]}\")"
      ],
      "metadata": {
        "id": "gZWhmbXNHtbV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test out character vectorizer\n",
        "random_train_chars = random.choice(train_chars)\n",
        "print(f\"Charified text:\\n{random_train_chars}\")\n",
        "print(f\"\\nLength of chars: {len(random_train_chars.split())}\")\n",
        "vectorized_chars = char_vectorizer([random_train_chars])\n",
        "print(f\"\\nVectorized chars:\\n{vectorized_chars}\")\n",
        "print(f\"\\nLength of vectorized chars: {len(vectorized_chars[0])}\")"
      ],
      "metadata": {
        "id": "A6G5_LnJICJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Character level embedding\n",
        "char_embed = layers.Embedding(input_dim=NUM_CHAR_TOKENS, # number of different characters\n",
        "                              output_dim=25,             # embedding dimension of each character (same as Figure 1 in https://arxiv.org/pdf/1612.05251.pdf)\n",
        "                              mask_zero=False,           # don't use masks (this messes up model_5 if set to True)\n",
        "                              name=\"char_embed\")\n",
        "\n",
        "# Test out character embedding layer\n",
        "print(f\"Charified text (before vectorization and embedding):\\n{random_train_chars}\\n\")\n",
        "char_embed_example = char_embed(char_vectorizer([random_train_chars]))\n",
        "print(f\"Embedded chars (after vectorization and embedding):\\n{char_embed_example}\\n\")\n",
        "print(f\"Character embedding shape: {char_embed_example.shape}\")"
      ],
      "metadata": {
        "id": "xmMvu10XIEeo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make Conv1D on chars only\n",
        "inputs = layers.Input(shape=(1,), dtype=\"string\")\n",
        "char_vectors = char_vectorizer(inputs)\n",
        "char_embeddings = char_embed(char_vectors)\n",
        "x = layers.Conv1D(64, kernel_size=5, padding=\"same\", activation=\"relu\")(char_embeddings)\n",
        "x = layers.GlobalMaxPool1D()(x)\n",
        "outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
        "model_3 = tf.keras.Model(inputs=inputs,\n",
        "                         outputs=outputs,\n",
        "                         name=\"model_3_conv1D_char_embedding\")\n",
        "\n",
        "# Compile model\n",
        "model_3.compile(loss=\"categorical_crossentropy\",\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "aAkqCB3kIUBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the summary of conv1d_char_model\n",
        "model_3.summary()"
      ],
      "metadata": {
        "id": "5jdeA643oMth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create char datasets\n",
        "train_char_dataset = tf.data.Dataset.from_tensor_slices((train_chars, train_labels_one_hot)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "val_char_dataset = tf.data.Dataset.from_tensor_slices((val_chars, val_labels_one_hot)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "train_char_dataset"
      ],
      "metadata": {
        "id": "2-03uQ1LoQpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model on chars only\n",
        "model_3_history = model_3.fit(train_char_dataset,\n",
        "                              steps_per_epoch=int(0.1 * len(train_char_dataset)),\n",
        "                              epochs=10,\n",
        "                              validation_data=val_char_dataset,\n",
        "                              validation_steps=int(0.1 * len(val_char_dataset)))"
      ],
      "metadata": {
        "id": "exRn5dWMqgxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate model_3 on whole validation char dataset\n",
        "model_3.evaluate(val_char_dataset)"
      ],
      "metadata": {
        "id": "ir_CbQvPqkIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions with character model only\n",
        "model_3_pred_probs = model_3.predict(val_char_dataset)\n",
        "model_3_pred_probs"
      ],
      "metadata": {
        "id": "pSx8zhlnq3qf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert predictions to classes\n",
        "model_3_preds = tf.argmax(model_3_pred_probs, axis=1)\n",
        "model_3_preds"
      ],
      "metadata": {
        "id": "PxgEIpCgq6cb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate Conv1D char only model results\n",
        "model_3_results = calculate_results(y_true=val_labels_encoded,\n",
        "                                        y_pred=model_3_preds)\n",
        "model_3_results"
      ],
      "metadata": {
        "id": "TYVzW2Xxq9QD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.6 Model 4: Multi-modal model (Hybrid pre-trained token embeddings and character embeddings)"
      ],
      "metadata": {
        "id": "qIsTKdHCuWpC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Setup token inputs/model\n",
        "token_inputs = layers.Input(shape=[], dtype=tf.string, name=\"token_input\")\n",
        "token_embeddings = tf_hub_embedding_layer(token_inputs)\n",
        "token_output = layers.Dense(128, activation=\"relu\")(token_embeddings)\n",
        "token_model = tf.keras.Model(inputs=token_inputs, outputs=token_output)\n",
        "\n",
        "# 2. Setup char inputs/model\n",
        "char_inputs = layers.Input(shape=(1,), dtype=tf.string, name=\"char_input\")\n",
        "char_vectors = char_vectorizer(char_inputs)\n",
        "char_embeddings = char_embed(char_vectors)\n",
        "char_bi_lstm = layers.Bidirectional(layers.LSTM(25))(char_embeddings) # bi-LSTM shown in Figure 1 of https://arxiv.org/pdf/1612.05251.pdf\n",
        "char_model = tf.keras.Model(inputs=char_inputs, outputs=char_bi_lstm)\n",
        "\n",
        "# 3. Concatenate token and char inputs (create hybrid token embedding)\n",
        "token_char_concat = layers.Concatenate(name=\"token_char_hybrid\")([token_model.output, char_model.output])\n",
        "\n",
        "# 4. Create output layers - addition of dropout discussed in 4.2 of https://arxiv.org/pdf/1612.05251.pdf\n",
        "combined_dropout = layers.Dropout(0.5)(token_char_concat)\n",
        "combined_dense = layers.Dense(200, activation=\"relu\")(combined_dropout) # slightly different to Figure 1 due to different shapes of token/char embedding layers\n",
        "final_dropout = layers.Dropout(0.5)(combined_dense)\n",
        "output_layer = layers.Dense(num_classes, activation=\"softmax\")(final_dropout)\n",
        "\n",
        "# 5. Construct model with char and token inputs\n",
        "model_4 = tf.keras.Model(inputs=[token_model.input, char_model.input],\n",
        "                         outputs=output_layer,\n",
        "                         name=\"model_4_token_and_char_embeddings\")"
      ],
      "metadata": {
        "id": "AGhOdJpOux7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get summary of token and character model\n",
        "model_4.summary()"
      ],
      "metadata": {
        "id": "Q3V2ud627Tz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot hybrid token and character model\n",
        "plot_model(model_4, show_shapes = True)"
      ],
      "metadata": {
        "id": "Pli3xCg07W-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile token char model\n",
        "model_4.compile(loss=\"categorical_crossentropy\",\n",
        "                optimizer=tf.keras.optimizers.Adam(), # section 4.2 of https://arxiv.org/pdf/1612.05251.pdf mentions using SGD but we'll stick with Adam\n",
        "                metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "BWfvZEpj7ZsC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combining the token and character into the dataset\n",
        "train_char_token_data = tf.data.Dataset.from_tensor_slices((train_sentences, train_chars)) # make data\n",
        "train_char_token_labels = tf.data.Dataset.from_tensor_slices(train_labels_one_hot) # make labels\n",
        "train_char_token_dataset = tf.data.Dataset.zip((train_char_token_data, train_char_token_labels)) # combine data and labels\n",
        "\n",
        "# Prefetch and batch train data\n",
        "train_char_token_dataset = train_char_token_dataset.batch(32).prefetch(tf.data.AUTOTUNE) \n",
        "\n",
        "# Repeat same steps validation data\n",
        "val_char_token_data = tf.data.Dataset.from_tensor_slices((val_sentences, val_chars))\n",
        "val_char_token_labels = tf.data.Dataset.from_tensor_slices(val_labels_one_hot)\n",
        "val_char_token_dataset = tf.data.Dataset.zip((val_char_token_data, val_char_token_labels))\n",
        "val_char_token_dataset = val_char_token_dataset.batch(32).prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "5KBdrrQ57cKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check out training char and token embedding dataset\n",
        "train_char_token_dataset, val_char_token_dataset"
      ],
      "metadata": {
        "id": "5S2vqkEC7kFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting the model\n",
        "model_4_history = model_4.fit(train_char_token_dataset, # train on dataset of token and characters\n",
        "                              steps_per_epoch=int(0.1 * len(train_char_token_dataset)),\n",
        "                              epochs=3,\n",
        "                              validation_data=val_char_token_dataset,\n",
        "                              validation_steps=int(0.1 * len(val_char_token_dataset)))"
      ],
      "metadata": {
        "id": "7_0-Kwh17nub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on the whole validation dataset\n",
        "model_4.evaluate(val_char_token_dataset)"
      ],
      "metadata": {
        "id": "f5ate_Xl7sC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions using the token-character model hybrid\n",
        "model_4_pred_probs = model_4.predict(val_char_token_dataset)\n",
        "model_4_pred_probs"
      ],
      "metadata": {
        "id": "B2QpCnX87ySG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn prediction probabilities into prediction classes\n",
        "model_4_preds = tf.argmax(model_4_pred_probs, axis=1)\n",
        "model_4_preds"
      ],
      "metadata": {
        "id": "gKL-QyQ4705A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get results of token-char-hybrid model\n",
        "model_4_results = calculate_results(y_true=val_labels_encoded,\n",
        "                                    y_pred=model_4_preds)\n",
        "model_4_results"
      ],
      "metadata": {
        "id": "r7TwHoH072hV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.7 Model 5 - Feature extraction transfer learning model (pre-trained token , character and positional embeddings):"
      ],
      "metadata": {
        "id": "dOiGro231j8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Trying to feature enginner the order into likehood of a particular class.\n",
        "# Inspect training dataframe\n",
        "train_df.head()"
      ],
      "metadata": {
        "id": "-sQad-nQ1jUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.7.1 Create positinoal embeddings"
      ],
      "metadata": {
        "id": "z5f3_aAE-oH4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# How many different line numbers are there?\n",
        "train_df[\"line_number\"].value_counts()"
      ],
      "metadata": {
        "id": "PUJus61S-lKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the distribution of \"line_number\" column\n",
        "train_df.line_number.plot.hist()"
      ],
      "metadata": {
        "id": "EiJaaaqj-uzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use TensorFlow to create one-hot-encoded tensors of our \"line_number\" column \n",
        "train_line_numbers_one_hot = tf.one_hot(train_df[\"line_number\"].to_numpy(), depth=15)\n",
        "val_line_numbers_one_hot = tf.one_hot(val_df[\"line_number\"].to_numpy(), depth=15)\n",
        "test_line_numbers_one_hot = tf.one_hot(test_df[\"line_number\"].to_numpy(), depth=15)"
      ],
      "metadata": {
        "id": "BojAJzHS-w6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check one-hot encoded \"line_number\" feature samples\n",
        "train_line_numbers_one_hot.shape, train_line_numbers_one_hot[:20]"
      ],
      "metadata": {
        "id": "XJyDpxJA-0EZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How many different numbers of lines are there?\n",
        "train_df[\"total_lines\"].value_counts()"
      ],
      "metadata": {
        "id": "iJJAewLK-8OS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the distribution of total lines\n",
        "train_df.total_lines.plot.hist();"
      ],
      "metadata": {
        "id": "CZG3KEmm--Ad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the coverage of a \"total_lines\" value of 20\n",
        "np.percentile(train_df.total_lines, 98) # a value of 20 covers 98% of samples"
      ],
      "metadata": {
        "id": "ht30goA6_ATV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use TensorFlow to create one-hot-encoded tensors of our \"total_lines\" column \n",
        "train_total_lines_one_hot = tf.one_hot(train_df[\"total_lines\"].to_numpy(), depth=20)\n",
        "val_total_lines_one_hot = tf.one_hot(val_df[\"total_lines\"].to_numpy(), depth=20)\n",
        "test_total_lines_one_hot = tf.one_hot(test_df[\"total_lines\"].to_numpy(), depth=20)\n",
        "\n",
        "# Check shape and samples of total lines one-hot tensor\n",
        "train_total_lines_one_hot.shape, train_total_lines_one_hot[:10]"
      ],
      "metadata": {
        "id": "Bzzj7SNH_Bfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.7.2 Creating a hybrid embedding model"
      ],
      "metadata": {
        "id": "W5kYE05PAQOY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Token inputs\n",
        "token_inputs = layers.Input(shape=[], dtype=\"string\", name=\"token_inputs\")\n",
        "token_embeddings = tf_hub_embedding_layer(token_inputs)\n",
        "token_outputs = layers.Dense(128, activation=\"relu\")(token_embeddings)\n",
        "token_model = tf.keras.Model(inputs=token_inputs, outputs=token_outputs)\n",
        "\n",
        "# 2. Char inputs\n",
        "char_inputs = layers.Input(shape=(1,), dtype=\"string\", name=\"char_inputs\")\n",
        "char_vectors = char_vectorizer(char_inputs)\n",
        "char_embeddings = char_embed(char_vectors)\n",
        "char_bi_lstm = layers.Bidirectional(layers.LSTM(32))(char_embeddings)\n",
        "char_model = tf.keras.Model(inputs=char_inputs, outputs=char_bi_lstm)\n",
        "\n",
        "# 3. Line numbers inputs\n",
        "line_number_inputs = layers.Input(shape=(15,), dtype=tf.int32, name=\"line_number_input\")\n",
        "x = layers.Dense(32, activation=\"relu\")(line_number_inputs)\n",
        "line_number_model = tf.keras.Model(inputs=line_number_inputs, outputs=x)\n",
        "\n",
        "# 4. Total lines inputs\n",
        "total_lines_inputs = layers.Input(shape=(20,), dtype=tf.int32, name=\"total_lines_input\")\n",
        "y = layers.Dense(32, activation=\"relu\")(total_lines_inputs)\n",
        "total_line_model = tf.keras.Model(inputs=total_lines_inputs, outputs=y)\n",
        "\n",
        "# 5. Combine token and char embeddings into a hybrid embedding\n",
        "combined_embeddings = layers.Concatenate(name=\"token_char_hybrid_embedding\")([token_model.output, char_model.output])\n",
        "z = layers.Dense(256, activation=\"relu\")(combined_embeddings)\n",
        "z = layers.Dropout(0.5)(z)\n",
        "\n",
        "# 6. Combine positional embeddings with combined token and char embeddings into a tribrid embedding\n",
        "z = layers.Concatenate(name=\"token_char_positional_embedding\")([line_number_model.output, total_line_model.output, z])\n",
        "\n",
        "# 7. Create output layer\n",
        "output_layer = layers.Dense(5, activation=\"softmax\", name=\"output_layer\")(z)\n",
        "\n",
        "# 8. Put together model\n",
        "model_5 = tf.keras.Model(inputs=[line_number_model.input,\n",
        "                                 total_line_model.input,\n",
        "                                 token_model.input, \n",
        "                                 char_model.input],\n",
        "                         outputs=output_layer)"
      ],
      "metadata": {
        "id": "-iOP1fI_AVL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a summary of our token, char and positional embedding model\n",
        "model_5.summary()"
      ],
      "metadata": {
        "id": "elKpz9BzDRsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the token, char, positional embedding model\n",
        "from tensorflow.keras.utils import plot_model\n",
        "plot_model(model_5)"
      ],
      "metadata": {
        "id": "57JPuhcDArU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check which layers of our model are trainable or not\n",
        "for layer in model_5.layers:\n",
        "  print(layer, layer.trainable)"
      ],
      "metadata": {
        "id": "t8pMo8N4Aus-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create training and validation datasets (all four kinds of inputs)\n",
        "train_pos_char_token_data = tf.data.Dataset.from_tensor_slices((train_line_numbers_one_hot, # line numbers\n",
        "                                                                train_total_lines_one_hot, # total lines\n",
        "                                                                train_sentences, # train tokens\n",
        "                                                                train_chars)) # train chars\n",
        "train_pos_char_token_labels = tf.data.Dataset.from_tensor_slices(train_labels_one_hot) # train labels\n",
        "train_pos_char_token_dataset = tf.data.Dataset.zip((train_pos_char_token_data, train_pos_char_token_labels)) # combine data and labels\n",
        "train_pos_char_token_dataset = train_pos_char_token_dataset.batch(32).prefetch(tf.data.AUTOTUNE) # turn into batches and prefetch appropriately\n",
        "\n",
        "# Validation dataset\n",
        "val_pos_char_token_data = tf.data.Dataset.from_tensor_slices((val_line_numbers_one_hot,\n",
        "                                                              val_total_lines_one_hot,\n",
        "                                                              val_sentences,\n",
        "                                                              val_chars))\n",
        "val_pos_char_token_labels = tf.data.Dataset.from_tensor_slices(val_labels_one_hot)\n",
        "val_pos_char_token_dataset = tf.data.Dataset.zip((val_pos_char_token_data, val_pos_char_token_labels))\n",
        "val_pos_char_token_dataset = val_pos_char_token_dataset.batch(32).prefetch(tf.data.AUTOTUNE) # turn into batches and prefetch appropriately\n",
        "\n",
        "# Check input shapes\n",
        "train_pos_char_token_dataset, val_pos_char_token_dataset"
      ],
      "metadata": {
        "id": "uEq6Wa9zA3N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the callbacks \n",
        "check_filepath = 'best_weights/checkpoint.ckpt'\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath= check_filepath , \n",
        "                                                               save_weights_only = True , \n",
        "                                                               save_best_only = True  , \n",
        "                                                               save_freq = 'epoch' , \n",
        "                                                               monitor = 'val_loss')\n",
        "\n",
        "early_stopping  = tf.keras.callbacks.EarlyStopping(monitor= 'val_loss' , \n",
        "                                                   patience = 3, min_delta = 0.5 , verbose = 1)\n",
        "\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\",  \n",
        "                                                 factor=0.2, \n",
        "                                                 patience=2,\n",
        "                                                 verbose=1, \n",
        "                                                 min_lr=1e-7)"
      ],
      "metadata": {
        "id": "wrIjn0oBUtsG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the token, char and positional embedding model\n",
        "model_5.compile(loss = tf.keras.losses.CategoricalCrossentropy(label_smoothing= 0.2) ,\n",
        "                    optimizer = tf.keras.optimizers.Adam() ,\n",
        "                metrics = ['accuracy'])\n",
        "\n",
        "history_model_5 = model_5.fit(train_pos_char_token_dataset,\n",
        "                              steps_per_epoch=int(0.1 * len(train_pos_char_token_dataset)),\n",
        "                              epochs=5,\n",
        "                              validation_data = val_pos_char_token_dataset, \n",
        "                              callbacks = [early_stopping , model_checkpoint_callback , reduce_lr])"
      ],
      "metadata": {
        "id": "UPw-TmbyBAKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions with token-char-positional hybrid model\n",
        "model_5_pred_probs = model_5.predict(val_pos_char_token_dataset, verbose=1)\n",
        "model_5_pred_probs"
      ],
      "metadata": {
        "id": "_6TZL5FNP0WG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn prediction probabilities into prediction classes\n",
        "model_5_preds = tf.argmax(model_5_pred_probs, axis=1)\n",
        "model_5_preds"
      ],
      "metadata": {
        "id": "clUNJjl1P1Bq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate results of token-char-positional hybrid model\n",
        "model_5_results = calculate_results(y_true=val_labels_encoded,\n",
        "                                    y_pred=model_5_preds)\n",
        "model_5_results"
      ],
      "metadata": {
        "id": "6Qw0lXrgP3FA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Comparing models"
      ],
      "metadata": {
        "id": "rgX9JK7BQB9J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine model results into a DataFrame\n",
        "all_model_results = pd.DataFrame({\"baseline\": baseline_results,\n",
        "                                  \"custom_token_embed_conv1d\": model_1_results,\n",
        "                                  \"pretrained_token_embed\": model_2_results,\n",
        "                                  \"custom_char_embed_conv1d\": model_3_results,\n",
        "                                  \"hybrid_char_token_embed\": model_4_results,\n",
        "                                  \"tribrid_pos_char_token_embed\": model_5_results})\n",
        "all_model_results = all_model_results.transpose()\n",
        "all_model_results"
      ],
      "metadata": {
        "id": "dXxeOopdQGHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reduce the accuracy to same scale as other metrics\n",
        "all_model_results[\"accuracy\"] = all_model_results[\"accuracy\"]/100"
      ],
      "metadata": {
        "id": "2MNStHRmQHzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot and compare all of the model results\n",
        "all_model_results.plot(kind=\"bar\", figsize=(10, 7)).legend(bbox_to_anchor=(1.0, 1.0));"
      ],
      "metadata": {
        "id": "xumBS8W1QLna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort model results by f1-score\n",
        "all_model_results.sort_values(\"f1\", ascending=False)[\"f1\"].plot(kind=\"bar\", figsize=(10, 7));"
      ],
      "metadata": {
        "id": "RvR2CgDvQND-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Make predictions"
      ],
      "metadata": {
        "id": "lBLrABTjSxO0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download and open example abstracts (copy and pasted from PubMed)\n",
        "!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/skimlit_example_abstracts.json\n",
        "\n",
        "import json\n",
        "with open(\"skimlit_example_abstracts.json\", \"r\") as f:\n",
        "  example_abstracts = json.load(f)\n",
        "\n",
        "example_abstracts"
      ],
      "metadata": {
        "id": "q_CrUXIvSzQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# See what our example abstracts look like\n",
        "abstracts = pd.DataFrame(example_abstracts)\n",
        "abstracts"
      ],
      "metadata": {
        "id": "pxpD9bnLS2f5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How does our abstracts look in a DataFrame? \n",
        "pd.DataFrame(example_abstracts)"
      ],
      "metadata": {
        "id": "5KGxE9NoS9Fw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_pred_sequence_labels(abstract_dict , model , label_encoder):\n",
        "\n",
        "  '''\n",
        "  \n",
        "    Takes in a list of dictionaries of abstracts, \n",
        "\n",
        "    [{'abstract': 'This RCT examined .......' , \n",
        "      'details': 'RCT of a manuali......',\n",
        "      'source': 'https://pubmed.ncbi.nlm........./'},..........] \n",
        "\n",
        "    Arguments: \n",
        "    ----------\n",
        "      - abstract_dict : Abstract dictionary of the above format \n",
        "      - model : the trained model on the same data format (line_numbers,  total_lines , sentences , characters)\n",
        "      - label_encoder : the label encoder used to encode the classes \n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "      Prints out the predicted label and the corresponding sequence/ text \n",
        "  '''\n",
        "\n",
        "  # Setup english sentence parser \n",
        "  nlp = English()\n",
        "\n",
        "  # Create sentence splitting pipeline object \n",
        "  # sentencizer = nlp.create_pipe('sentencizer')\n",
        "  nlp.add_pipe('sentencizer')\n",
        "\n",
        "  # Create doc of parsed sequences\n",
        "  doc = nlp(abstract_dict[0]['abstract'])\n",
        "\n",
        "  # Return detected sentences from doc in string typpe \n",
        "  abstract_lines = [str(sent) for sent in list(doc.sents)]\n",
        "\n",
        "  # Get total number of lines \n",
        "  total_lines_in_sample = len(abstract_lines)\n",
        "\n",
        "  # Loop through each line in the abstract and create a list of dictionaries containing features \n",
        "  sample_lines = []\n",
        "  for i , line in enumerate(abstract_lines):\n",
        "    sample_dict = {}\n",
        "    sample_dict['text'] = str(line)\n",
        "    sample_dict['line_number'] = i \n",
        "    sample_dict['total_lines'] = total_lines_in_sample - 1 \n",
        "    sample_lines.append(sample_dict)\n",
        "\n",
        "  \n",
        "  # Get all line number and total lines numbers then one hot encode them \n",
        "  abstract_line_numbers = [line['line_number'] for line in sample_lines]\n",
        "  abstract_total_lines = [line['total_lines'] for line in sample_lines]\n",
        "\n",
        "  abstract_line_numbers_one_hot = tf.one_hot(abstract_line_numbers , depth = 15)\n",
        "  abstract_total_lines_one_hot = tf.one_hot(abstract_total_lines , depth = 20)\n",
        "\n",
        "\n",
        "  # Split the lines into characters \n",
        "  abstract_chars = [split_chars(sentence) for sentence in abstract_lines]\n",
        "\n",
        "  # Making prediction on sample features\n",
        "  abstract_pred_probs = model.predict(x = (abstract_line_numbers_one_hot, \n",
        "                                           abstract_total_lines_one_hot , \n",
        "                                           tf.constant(abstract_lines) , \n",
        "                                           tf.constant(abstract_chars)))\n",
        "  \n",
        "  # Turn prediction probs to pred class \n",
        "  abstract_preds = tf.argmax(abstract_pred_probs , axis = 1)\n",
        "  \n",
        "  # Prediction class integers into string class name \n",
        "  abstract_pred_classes = [label_encoder.classes_[i] for i in abstract_preds]\n",
        "\n",
        "  # Prints out the abstract lines and the predicted sequence labels \n",
        "  for i , line in enumerate(abstract_lines):\n",
        "    print(f'{abstract_pred_classes[i]}:  {line}\\n')"
      ],
      "metadata": {
        "id": "DH56D5H4dAw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_pred_sequence_labels(example_abstracts , model_5 , label_encoder)"
      ],
      "metadata": {
        "id": "og29gWU8WxmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/Srihari293/AI_ML/main/Projects/skim_lit/adi_abstract.json\n",
        "with open(\"adi_abstract.json\", \"r\") as f:\n",
        "  adi_abstracts = json.load(f)\n",
        "visualize_pred_sequence_labels(adi_abstracts , model_5 , label_encoder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7XxTXV6Wkhe4",
        "outputId": "dce7c18e-90f5-4433-98bd-6c5c098d8049"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-07 06:31:58--  https://raw.githubusercontent.com/Srihari293/AI_ML/main/Projects/skim_lit/adi_abstract.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1776 (1.7K) [text/plain]\n",
            "Saving to: adi_abstract.json.2\n",
            "\n",
            "\radi_abstract.json.2   0%[                    ]       0  --.-KB/s               \radi_abstract.json.2 100%[===================>]   1.73K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-02-07 06:31:58 (32.2 MB/s) - adi_abstract.json.2 saved [1776/1776]\n",
            "\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "BACKGROUND:  Attention deficit/hyperactivity disorder (ADHD) is associated with socioeconomic status (SES), in that children who grow up in low SES families are at an increased risk of ADHD symptoms and diagnosis.\n",
            "\n",
            "BACKGROUND:  The current study explores whether different levels of ADHD symptoms are associated with prior changes in the SES facet of financial difficulty.\n",
            "\n",
            "METHODS:  Using the Avon Longitudinal Study of Parents and Children (ALSPAC), we examined symptoms of ADHD measured by the Strengths and Difficulties Questionnaire (SDQ) hyperactivity subscale in relation to parent-reported changes in financial difficulty, grouped into four repeated measures at four time points across childhood; (n = 6416).\n",
            "\n",
            "METHODS:  A multilevel mixed-effects linear regression model with an unstructured covariance matrix was used to test whether different patterns of financial difficulty were associated with subsequent changes in ADHD symptoms.\n",
            "\n",
            "RESULTS:  Families who had no financial difficulty had children with a lower average ADHD symptom score than groups who experienced financial difficulty.\n",
            "\n",
            "RESULTS:  Children whose families stayed in financial difficulty had higher mean ADHD symptom scores than all other groups (No difficulty mean SDQ hyperactivity 3.14, 95% CI 3.07, 3.21, In difficulty mean SDQ hyperactivity 3.39, 95% CI 3.28, 3.45, p < 0.001).\n",
            "\n",
            "RESULTS:  Increasing or decreasing financial difficulty predicted mean symptom scores lower than those of the in difficulty group and higher than the no difficulty group.\n",
            "\n",
            "CONCLUSIONS:  Our findings contribute to the building evidence that SES may influence the severity and/or impairment associated with the symptoms of ADHD, however the effects of SES are small and have limited clinical significance.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P1MKSUjykrmk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}